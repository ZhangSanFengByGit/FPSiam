


SiamRPN_init use time 0.022411108017


SiamRPN_train use time 0.788831949234


backward use time 1.78012990952


optimizer.step() use time 0.00104212760925




/home/zhangzichun/FFPSiamR/code/run_SiamRPN.py:345: RuntimeWarning: invalid value encountered in subtract
  boxAArea = (proposals_box[2,:] - proposals_box[0,:] + 1) * (proposals_box[3,:] - proposals_box[1,:] + 1)
/home/zhangzichun/FFPSiamR/code/run_SiamRPN.py:98: RuntimeWarning: invalid value encountered in greater_equal
  positive = np.greater_equal(iou, p.pos_th)
/home/zhangzichun/FFPSiamR/code/run_SiamRPN.py:99: RuntimeWarning: invalid value encountered in less
  negative = np.less(iou, p.pos_th)
/home/zhangzichun/FFPSiamR/code/run_SiamRPN.py:114: RuntimeWarning: invalid value encountered in multiply
  delta_gt = delta_np_pos*np.tile(positive, (4,1)) + delta_np*np.tile(negative, (4,1))




 epoch:56 seq:Board	cls_loss:0.00675447937101 , box_loss:0.362794041634
/home/zhangzichun/FFPSiamR/code/run_SiamRPN.py:112: RuntimeWarning: divide by zero encountered in log
  score_gt[0,max_pos] = 0
/home/zhangzichun/FFPSiamR/code/run_SiamRPN.py:113: RuntimeWarning: divide by zero encountered in log
  score_gt = torch.from_numpy(score_gt).cuda()
/home/zhangzichun/FFPSiamR/code/run_SiamRPN.py:115: RuntimeWarning: invalid value encountered in multiply
  delta_np_pos = np.zeros(delta_np.shape)
epoch:56 seq:Board	cls_loss:0.000780690577812 , box_loss:nan
/home/zhangzichun/FFPSiamR/code/run_SiamRPN.py:348: RuntimeWarning: invalid value encountered in subtract
  inter[0,:] = np.maximum(proposals_box[0,:], boxB[0])
/home/zhangzichun/FFPSiamR/code/run_SiamRPN.py:99: RuntimeWarning: invalid value encountered in greater_equal
  negative = np.less(iou, p.pos_th)
/home/zhangzichun/FFPSiamR/code/run_SiamRPN.py:100: RuntimeWarning: invalid value encountered in less
  positive_pos = np.argwhere(positive==1)
epoch:56 seq:Board	cls_loss:nan , box_loss:nan






process0 fixed propagation weight
process1 add feat extractor etc into traning     FAIL
process2 1 add sigma to log loss prevent inf grade  2 add the feat extractor e.t.c into traning

process1 1 add sigma to log loss prevent inf grade  2 add the feat extractor e.t.c into traning
          and use nohup





Encounter 'inf' has no connection with training <feat extractor etc>



Special ground truth:
(zzc_env2.7) zhangzichun@cvpruser:~/FFPSiamR/code$ python otbjiaoben.py 
Biker
Crowds
Board
Twinnings


VV1:
{
V1 : process1 1 add sigma to log loss prevent inf grade  2 add the feat extractor e.t.c into traning
          and use nohup]

V2 : lambda-->10   general lr-->0.001]

V3 : warm up 255-->271      remove the 'assert' about log(gt/anchor) around line 149]
}



VV2:
{
  V1  : due to it's wrong to train batchNorm2d with batch size=1, we fixed all the featureExtractor when trainning
        general learning rate -->0.0001]
}


VV3:
{
  V1 : big change: <update to batch training mode>

  V2 : track_running_stats=False, to frozen the running mean and var in the previous batchnorms (test to be False later)

  V3 : fuck the origin warm up which get nan to the net, plus increase learning rate to 0.1

  V4 : try to use smaller batch size since 64 is driving crazy, and I used 32

  V5 : try to use larger Lambda:100 in the balancing of box loss and class loss,
	and I have found there are some thing wrong in my lr decay method that the lr is actually not decaying


  V6 2019.1.27: when use large lr for the fine-tunning last 4 layers of the original networks,   always encountered overflow and nan in several epoch
  
}













